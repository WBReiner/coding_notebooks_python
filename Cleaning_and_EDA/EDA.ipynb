{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Quick Reference\n",
    "\n",
    "## Why is EDA important?\n",
    "Some of the reasons EDA is a good use of time:\n",
    "\n",
    "* Identify patterns\n",
    "* Build up an intuition about the data BASED ON the data, not the description of what the data is supposed to be/show.\n",
    "** Leads to/informs:\n",
    "*** Development of hypotheses\n",
    "*** Model selection\n",
    "*** Feature engineering\n",
    "\n",
    "\n",
    "**Reasons for client/CEO/manager/boss/etc. (the audience)**\n",
    "* Helps to ensure:\n",
    "** that the results are technically sound and based on the data\n",
    "** that the right questions are being asked\n",
    "* Can help uncover other question that should be asked\n",
    "* Tests assumptions about the data and the business problem(s)\n",
    "* Provides context for the application of the results.\n",
    "* May underscore the value of the results.\n",
    "* Can lead to new insights that would otherwise not be found or new avenues by which additional insights can be gained through additional modeling or the collection of additional data.\n",
    "\n",
    "## Remember\n",
    "* EDA is never something that gets finished- with every analytical result, it is important to return to EDA to:\n",
    "* Make sure the result makes sense according to the data and the problem/questions\n",
    "* Test new questions that arise from the results  \n",
    "* Be as objective as possible and listen to the DATA, not your assumptions or the company's assumptions about the data: the goal is to challenge and evaluate the assumptions. \n",
    "* Repeat EDA for every new problem even if the data remain the same. New perspectives (via new problems/goals) may reveal new insights about the data.\n",
    "\n",
    "## Major tasks of EDA\n",
    "Presented linearly, but in reality you will not follow this order exactly; the data and your choices based on the problem(s) and time-constraints will determine the order. \n",
    "\n",
    "1. Formulate hypothesis/develop investigation themes to explore/understand question and assumptions about data and what it would look like if these assumptions are met. \n",
    "2. Clean and wrangle data \n",
    "3. Assess data quality\n",
    "4. Summarize data \n",
    "5. Explore each individual variable in the dataset \n",
    "6. Assess relationships/interactions:\n",
    "   a. between each variable and/or target or goal/problem (if not predictive)\n",
    "   b. between variables \n",
    "8. Explore the data across multiple dimensions \n",
    "\n",
    "Throughout analysis:\n",
    "* Capture a list of hypotheses and questions that come up that might merit further exploration.\n",
    "* Record what to watch out for/ be aware of\n",
    "* Show intermediate results, get domain expertise from others, re-form perspective\n",
    "* Pair visualizations and results to maximize ROI.\n",
    "\n",
    "## Cleaning/Wrangling \n",
    "\n",
    "### Basic things to do \n",
    "* Make your data [tidy](https://tomaugspurger.github.io/modern-5-tidy.html).\n",
    "    1. Each variable forms a column\n",
    "    2. Each observation forms a row\n",
    "    3. Each type of observational unit forms a table\n",
    "* Transform data: sometimes you will need to transform your data to be able to extract information from it. This step will usually occur after some of the other steps of EDA unless domain knowledge can inform these choices beforehand.  \n",
    "    * Log: when data is highly skewed (versus normally distributed like a bell curve), sometimes it has a log-normal distribution and taking the log of each data point will normalize it. \n",
    "    * Binning of continuous variables: Binning continuous variables and then analyzing the groups of observations created can allow for easier pattern identification. Especially with non-linear relationships. \n",
    "    * Simplifying of categories: you really don't want more than 8-10 categories within a single data field. Try to aggregate to higher-level categories when it makes sense.\n",
    "    \n",
    "Above test inspired by and adapted from https://github.com/cmawer/pycon-2017-eda-tutorial/blob/master/EDA-cheat-sheet.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniel's EDA notebook\n",
    "Exploratory Data Analysis\n",
    "This notebook contains boilerplate code for typical EDA steps:\n",
    "\n",
    "Import\n",
    "Summary Stats\n",
    "Cleaning\n",
    "Missing data\n",
    "Exploratory plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a csv file without the annoying index column\n",
    "a = pd.read_csv(\"a.csv\", index_col = 0)\n",
    "\n",
    "# Naming columns for a dataframe\n",
    "colnames= ['column1', 'column2', 'etc']\n",
    "a.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining dataframes, SQL style\n",
    "df = a.join(b, how = 'left', on = 'key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing & cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type per column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's often helpful to have a count of unique values in each column:\n",
    "def nunicol(df):\n",
    "    summary = []\n",
    "    for i in range(0, len(df.columns)):\n",
    "        summary.append(df.iloc[:,i].nunique())\n",
    "    \n",
    "    summary = pd.DataFrame([summary])\n",
    "    summary.columns = df.columns\n",
    "    \n",
    "    return summary\n",
    "\n",
    "nunicol(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make crosstab table for initial overview. Also exposes misspelled feature levels.\n",
    "ct = pd.crosstab([df.feature_1, df.feature_2, df.feature_3], df.target, normalize='index')\n",
    "ct.sort_values(by=1, ascending=False)\n",
    "\n",
    "# normalize by 'index' gives percentages per row\n",
    "# normalize by 'all' gives overall percentages\n",
    "# to access a column, use e.g.: ct.iloc[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing spellings/typos using a dictionary\n",
    "df.replace({'column_name' : { 'wrong_1' : 'correct_1', 'wrong_2': 'correct_2'}},\n",
    "           inplace=True)\n",
    "\n",
    "# display levels after replacing misspellings\n",
    "df.column_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas aggregation with only one function:\n",
    "df_by_color = df_nn.groupby(['color'])['yc_g', 'yr_g', 'rc_g', 'red'].mean()\n",
    "                                                \n",
    "# to convert hierarchical index to normal dataframe\n",
    "df_by_color = df_by_color.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count NaNs in dataframe by column\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing rows or columns with NaNs\n",
    "df.dropna(axis=0, inplace=True) # axis=0 for rows, axis=1 for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean imputation\n",
    "df.column.fillna(df.column.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation using sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean') # or 'median', 'most_frequent',\n",
    "# 'constant'\n",
    "# filling the dataframe\n",
    "df_imped = imp.fit_transform(df)\n",
    "\n",
    "# when dealing with separate train/test sets, carry out fit and transfor separately:\n",
    "imp.fit(X_train)\n",
    "X_test = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms, PDFs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 26 10:23:45 2018\n",
    "\n",
    "@author: whitneyreiner\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# Histograms\n",
    "# From: https://realpython.com/python-histograms/\n",
    "# histograms: tool for quickly assessing a probability distribution. \n",
    "# https://github.com/realpython/materials/blob/master/histograms/histograms.py\n",
    "# =============================================================================\n",
    "#%%\n",
    "# =============================================================================\n",
    "# Need not be sorted, necessarily\n",
    "a = (0, 1, 1, 1, 2, 3, 7, 7, 23)\n",
    "\n",
    "#make a dictionary:\n",
    "def count_elements(seq) -> dict:\n",
    "# Tally elements from `seq`.\n",
    "    hist = {}\n",
    "    for i in seq:\n",
    "        hist[i] = hist.get(i, 0) + 1 # “for each element of the sequence, \n",
    "        # increment its corresponding value in hist by 1.”\n",
    "    return hist\n",
    "\n",
    "counted = count_elements(a)\n",
    "counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# =============================================================================\n",
    "# In fact, this is precisely what is done by the collections.Counter class from \n",
    "# Python’s standard library, which subclasses a Python dictionary and overrides \n",
    "# its .update() method:\n",
    "# =============================================================================\n",
    "#%%\n",
    "# Can also use counter\n",
    "from collections import Counter\n",
    "\n",
    "recounted = Counter(a)\n",
    "recounted\n",
    "#test the two are equal\n",
    "recounted.items() == counted.items()\n",
    "\n",
    "        \n",
    "import random\n",
    "random.seed(1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =============================================================================\n",
    "# Thus far, you have been working with what could best be called \n",
    "# “frequency tables.” But mathematically, a histogram is a mapping of bins \n",
    "# (intervals) to frequencies. More technically, it can be used to approximate the\n",
    "#  probability density function (PDF) of the underlying variable.\n",
    "# =============================================================================\n",
    "#%%\n",
    "# =============================================================================\n",
    "# Technical Detail: All but the last (rightmost) bin is half-open. That is, all \n",
    "# bins but the last are [inclusive, exclusive), and the final bin is\n",
    "#     [inclusive, inclusive].\n",
    "# =============================================================================\n",
    "#%%\n",
    "# =============================================================================\n",
    "# Staying in Python’s scientific stack, Pandas’ Series.histogram() uses \n",
    "# matplotlib.pyplot.hist() to draw a Matplotlib histogram of the input Series:\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Generate data on commute times.\n",
    "size, scale = 1000, 10\n",
    "commutes = pd.Series(np.random.gamma(scale, size=size) ** 1.5)\n",
    "\n",
    "commutes.plot.hist(grid=True, bins=20, rwidth=0.9,\n",
    "                   color='#607c8e')\n",
    "plt.title('Commute Times for 1,000 Commuters')\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Commute Time')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "#%%\n",
    "# =============================================================================\n",
    "# pandas.DataFrame.histogram() is similar but produces a histogram for each \n",
    "# column of data in the DataFrame.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# =============================================================================\n",
    "# A kernel density estimation (KDE) is a way to estimate the probability density \n",
    "# function (PDF) of the random variable that “underlies” our sample. KDE is a\n",
    "#  means of data smoothing.\n",
    "# =============================================================================\n",
    "#%%\n",
    "# Sample from two different normal distributions\n",
    "means = 10, 20\n",
    "stdevs = 4, 2\n",
    "dist = pd.DataFrame(np.random.normal(loc=means, scale=stdevs, size=(1000, 2)),\n",
    "                    columns=['a', 'b'])\n",
    "dist.agg(['min', 'max', 'mean', 'std']).round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, to plot each histogram on the same Matplotlib axes:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "dist.plot.kde(ax=ax, legend=False, title='Histogram: A vs. B')\n",
    "dist.plot.hist(density=True, ax=ax)\n",
    "ax.set_ylabel('Probability')\n",
    "ax.grid(axis='y')\n",
    "ax.set_facecolor('#d8dcd6')\n",
    "#%%\n",
    "# =============================================================================\n",
    "# If you take a closer look at this function, you can see how well it approximates the \n",
    "# “true” PDF for a relatively small sample of 1000 data points. Below, you can first build\n",
    "# the “analytical” distribution with scipy.stats.norm(). This is a class instance that \n",
    "# encapsulates the statistical standard normal distribution, its moments, and descriptive \n",
    "# functions. Its PDF is “exact” in the sense that it is defined precisely as \n",
    "# norm.pdf(x) = exp(-x**2/2) / sqrt(2*pi).\n",
    "# \n",
    "# Building from there, you can take a random sample of 1000 datapoints from this \n",
    "# distribution, then attempt to back into an estimation of the PDF with \n",
    "# scipy.stats.gaussian_kde():\n",
    "# \n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# An object representing the \"frozen\" analytical distribution\n",
    "# Defaults to the standard normal distribution, N~(0, 1)\n",
    "dist = stats.norm()\n",
    "\n",
    "# Draw random samples from the population you built above.\n",
    "# This is just a sample, so the mean and std. deviation should\n",
    "# be close to (1, 0).\n",
    "samp = dist.rvs(size=1000)\n",
    "\n",
    "# `ppf()`: percent point function (inverse of cdf — percentiles).\n",
    "x = np.linspace(start=stats.norm.ppf(0.01),\n",
    "                stop=stats.norm.ppf(0.99), num=250)\n",
    "gkde = stats.gaussian_kde(dataset=samp)\n",
    "\n",
    "# `gkde.evaluate()` estimates the PDF itself.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, dist.pdf(x), linestyle='solid', c='red', lw=3,\n",
    "        alpha=0.8, label='Analytical (True) PDF')\n",
    "ax.plot(x, gkde.evaluate(x), linestyle='dashed', c='black', lw=2,\n",
    "        label='PDF Estimated via KDE')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "ax.set_title('Analytical vs. Estimated PDF')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.text(-2., 0.35, r'$f(x) = \\frac{\\exp(-x^2/2)}{\\sqrt{2*\\pi}}$',\n",
    "        fontsize=12)\n",
    "#%%\n",
    "# =============================================================================\n",
    "# This is a bigger chunk of code, so let’s take a second to touch on a few key lines:\n",
    "# \n",
    "# SciPy’s stats subpackage lets you create Python objects that represent analytical \n",
    "# distributions that you can sample from to create actual data. So dist = stats.norm() \n",
    "# represents a normal continuous random variable, and you generate random numbers from it \n",
    "# with dist.rvs(). To evaluate both the analytical PDF and the Gaussian KDE, you need an \n",
    "# array x of quantiles (standard deviations above/below the mean, for a normal \n",
    "# distribution). stats.gaussian_kde() represents an estimated PDF that you need to \n",
    "# evaluate on an array to produce something visually meaningful in this case.\n",
    "# The last line contains some LaTex, which integrates nicely with Matplotlib.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# A Fancy Alternative with Seaborn\n",
    "# Let’s bring one more Python package into the mix. Seaborn has a displot() function that\n",
    "# plots the histogram and KDE for a univariate distribution in one step. Using the NumPy \n",
    "# array d from ealier:\n",
    "# \n",
    "# =============================================================================\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "# `numpy.random` uses its own PRNG.\n",
    "np.random.seed(444)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "d = np.random.laplace(loc=15, scale=3, size=500)\n",
    "d[:5]\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# The call above produces a KDE. There is also optionality to fit a specific distribution \n",
    "# to the data. This is different than a KDE and consists of parameter estimation for \n",
    "# generic data and a specified distribution name:\n",
    "# =============================================================================\n",
    "sns.distplot(d, fit=stats.laplace, kde=False)\n",
    "# =============================================================================\n",
    "# Again, note the slight difference. In the first case, you’re estimating some unknown \n",
    "# PDF; in the second, you’re taking a known distribution and finding what parameters best \n",
    "# describe it given the empirical data.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# In addition to its plotting tools, Pandas also offers a convenient .value_counts() \n",
    "# method that computes a histogram of non-null values to a Pandas Series:\n",
    "# \n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "\n",
    "data = np.random.choice(np.arange(10), size=10000, p=np.linspace(1, 11, 10) / 60)\n",
    "s = pd.Series(data)\n",
    "\n",
    "s.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.value_counts(normalize=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Elsewhere, pandas.cut() is a convenient way to bin values into arbitrary intervals.\n",
    "# Let’s say you have some data on ages of individuals and want to bucket them sensibly:\n",
    "# =============================================================================\n",
    "ages = pd.Series([1, 1, 3, 5, 8, 10, 12, 15, 18, 18, 19, 20, 25, 30, 40, 51, 52])\n",
    "bins = (0, 10, 13, 18, 21, np.inf)  # The edges\n",
    "labels = ('child', 'preteen', 'teen', 'military_age', 'adult')\n",
    "groups = pd.cut(ages, bins=bins, labels=labels)\n",
    "\n",
    "groups.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((ages, groups), axis=1).rename(columns={0: 'age', 1: 'group'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate distribution plot (Histogram with optional kde and rug plot)\n",
    "sns.distplot(df.column, kde=False, rug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot lines for  distributions\n",
    "def plot_with_fill(x, y, label):\n",
    "    lines = plt.plot(x, y, label=label, lw=2)\n",
    "    plt.fill_between(x, 0, y, alpha=0.2, color=lines[0].get_c())\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "feature_names = list(df.columns[1:10])\n",
    "label_name = list(df.columns[10:])\n",
    "\n",
    "features = df[feature_names]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(features.corr(), annot=True, square=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn Catplots\n",
    "import seaborn as sns\n",
    "\n",
    "#bar charts\n",
    "sns.catplot(x=\"col\", y=\"other_col\", kind=\"bar\", data=df.loc[df['col']!='what you do not want'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigger one\n",
    "sns.catplot(x=\"col\", y=\"other_col\", kind=\"bar\", data=df.loc[df['col']!='what you do not want'],size=7,aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot with catplot\n",
    "sns.catplot(x=\"col\", y=\"other_col\", kind=\"bar\", data=df.loc[df['col']!='what you do not want'],size=8,aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with hue\n",
    "sns.catplot(x=\"col\", y=\"other_col\", kind=\"bar\",hue='column_to_color_by', data=df.loc[df['col'\n",
    "            ]!='what_don_not_want'],size=8,aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "sns.scatterplot(x=\"col\", y=\"other_col\", kind=\"bar\", data=df.loc[df['col']='what you want'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facet wrapping/grid\n",
    "#Look at natalie morse's DC 3 for specifics and for more examples\n",
    "#salary groups and retention rate\n",
    "sns.set(font_scale=2)\n",
    "sns.catplot(\"df\", col=\"col_name\", col_wrap=4,\n",
    "data=og_df, kind=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "sns.distplot(df[df['quitters'] == 1]['salary'], kde=True, rug=False, label = \"Quit\")\n",
    "sns.distplot(df[df['quitters'] == 0]['salary'], kde=True, rug=False, label = \"No Quit\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# from Scott's DC3 submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# four at once:\n",
    "# Seniority\n",
    "fig, axs = plt.subplots(6,1, figsize = (6,20))\n",
    "i = 0\n",
    "for col in set(list(df['dept'])): # get unique dept\n",
    "    df_tmp = df[df['dept'] == col]\n",
    "    sns.distplot(df_tmp[df_tmp['quitters'] == 1]['seniority'], kde=True, rug=False, ax = axs[i], label = \"quit\")\n",
    "    sns.distplot(df_tmp[df_tmp['quitters'] == 0]['seniority'], kde=True, rug=False,\n",
    "                 ax = axs[i], axlabel= col, label = \"No quit\")\n",
    "    i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# from Scott's DC3 submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice faceted bar charts - natalia's DC3 submission\n",
    "\n",
    "import seaborn as sns\n",
    "g=sns.catplot(\"dept\",\"current\", col=\"company_id\", data=ec, kind=\"bar\", height=2.5, aspect=.8, col_wrap=6)\n",
    "g.set_xticklabels(rotation=30, ha='right')\n",
    "\n",
    "#I think this is only showing proportion of current employees in each department at each company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to my stuff (WBR)\n",
    "Graphs and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize distributions\n",
    "\n",
    "def plot_with_fill(x, y, label):\n",
    "\n",
    "    lines = plt.plot(x, y, label=label, lw=2)\n",
    "    plt.fill_between(x, 0, y, alpha=0.2, color=lines[0].get_c())\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To get pdf for beta distribution\n",
    "\n",
    "PDF is a function, whose value at any given sample (or point) in the sample space \n",
    "\n",
    "(the set of possible values taken by the random variable) can be interpreted as providing \n",
    "\n",
    "a relative likelihood that the value of the random variable would equal that sample.\n",
    "\n",
    "'''\n",
    "\n",
    "def get_pdf(x, site):\n",
    "\n",
    "    ''' \n",
    "\n",
    "    Parameters\n",
    "\n",
    "    -----------\n",
    "\n",
    "    x : Array of x values\n",
    "\n",
    "    site : Array cooresponding to the site in question\n",
    "    \n",
    "    Returns\n",
    "\n",
    "    --------\n",
    "    numpy array\n",
    "\n",
    "    '''\n",
    "\n",
    "    alpha = sum(site)\n",
    "    beta = len(site) - alpha\n",
    "    return scs.beta(a=alpha, b=beta).pdf(x)\n",
    "\n",
    "\"\"\"Start by looking only at converstion rate for old price.\n",
    "We assume a uniform prior, i.e., probability of 0 or 1 equally likely.\n",
    "Specifically, we use a beta distribution with alpha=1 and beta=1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a bunch of different plots at once\n",
    "\n",
    "features=[#column names go in here]\n",
    "\n",
    "fig=plt.subplots(figsize=(10,15))\n",
    "\n",
    "for i, j in enumerate(features):\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.subplots_adjust(hspace = 1.0)\n",
    "    sns.countplot(x=j,df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot\n",
    "sns.violinplot(x='color', y='red',  data=df_by_playerColor, inner='point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing (counts) after a group-by\n",
    "data6 = data2.groupby('skin_tone')['total_reds'].agg(['sum','count'])\n",
    "data6['percent'] = data6['sum'] / data6['count']\n",
    "data6.reset_index(inplace=True) # USEFUL HOW TO GRAPH GROUPBY STUFF IN FUTURE \n",
    "plt.figure(figsize=(17,3))\n",
    "plt.title('Frequency Distribution of Red Cards', fontsize = 14)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Skin Tone', fontsize=12)\n",
    "sns.barplot(x = \"skin_tone\", y = \"sum\", data = data6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graphing after a group-by with percentage\n",
    "data6 = data2.groupby('skin_tone')['total_reds'].agg(['sum','count'])\n",
    "data6['percent'] = data6['sum'] / data6['count']\n",
    "data6.reset_index(inplace=True) # USEFUL HOW TO GRAPH GROUPBY STUFF IN FUTURE \n",
    "plt.figure(figsize=(17,3))\n",
    "plt.title('Percentage Distribution of Red Cards', fontsize = 14)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Skin Tone', fontsize=12)\n",
    "sns.barplot(x = \"skin_tone\", y = \"percent\", data = data6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair-plots\n",
    "sns.set(style='white')\n",
    "sns.set(style='whitegrid', color_codes=True)\n",
    "\n",
    "sns.pairplot(player_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dist plots\n",
    "sns.distplot(player_data.iloc[:, 11])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "plt.scatter(player_data.iloc[:, 11], player_data.iloc[:, 8], marker='s', label='yellows')\n",
    "plt.show()\n",
    "plt.scatter(player_data.iloc[:, 11], player_data.iloc[:, 9], marker='o', label='yellowreds')\n",
    "plt.show()\n",
    "plt.scatter(player_data.iloc[:, 11], player_data.iloc[:, 10], marker='d', label='reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jointplot\n",
    "sns.jointplot(x = 'skinTone', y = 'RCRate', data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency bar chart\n",
    "plt.hist(df['avg_rate'])\n",
    "plt.xlabel(\"avg skin tone rating\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
